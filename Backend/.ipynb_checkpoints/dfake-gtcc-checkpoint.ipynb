{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b4a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization,Conv2D,MaxPooling2D,Activation,Dropout,Dense,Flatten,Input,Bidirectional,LSTM,MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, balanced_accuracy_score, classification_report\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, TimeDistributed, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7b3575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-0.55307</th>\n",
       "      <th>-13.949</th>\n",
       "      <th>7.2276</th>\n",
       "      <th>0.81332</th>\n",
       "      <th>0.81042</th>\n",
       "      <th>0.57891</th>\n",
       "      <th>0.057156</th>\n",
       "      <th>0.30691</th>\n",
       "      <th>-0.28209</th>\n",
       "      <th>0.34901</th>\n",
       "      <th>...</th>\n",
       "      <th>-1.2757e-05</th>\n",
       "      <th>-1.791e-05</th>\n",
       "      <th>0.00014175</th>\n",
       "      <th>0.00018104</th>\n",
       "      <th>2.7085e-05</th>\n",
       "      <th>1.9801e-05</th>\n",
       "      <th>1.6102e-05</th>\n",
       "      <th>1.5949e-05</th>\n",
       "      <th>-1.783e-05</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33518</td>\n",
       "      <td>-14.134</td>\n",
       "      <td>7.3409</td>\n",
       "      <td>0.99856</td>\n",
       "      <td>0.87461</td>\n",
       "      <td>0.806150</td>\n",
       "      <td>-0.024650</td>\n",
       "      <td>0.17291</td>\n",
       "      <td>-0.33139</td>\n",
       "      <td>0.296080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>2.032200e-04</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.53979</td>\n",
       "      <td>-14.600</td>\n",
       "      <td>7.5290</td>\n",
       "      <td>1.25120</td>\n",
       "      <td>0.65054</td>\n",
       "      <td>0.731180</td>\n",
       "      <td>0.036801</td>\n",
       "      <td>0.25653</td>\n",
       "      <td>-0.28908</td>\n",
       "      <td>0.337880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>5.168800e-07</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.96780</td>\n",
       "      <td>-15.274</td>\n",
       "      <td>7.1488</td>\n",
       "      <td>1.34540</td>\n",
       "      <td>0.83254</td>\n",
       "      <td>0.911210</td>\n",
       "      <td>0.125130</td>\n",
       "      <td>0.32822</td>\n",
       "      <td>-0.29095</td>\n",
       "      <td>0.342770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-2.219200e-05</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.12280</td>\n",
       "      <td>-12.564</td>\n",
       "      <td>7.0406</td>\n",
       "      <td>0.93019</td>\n",
       "      <td>1.26220</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0.570680</td>\n",
       "      <td>0.26390</td>\n",
       "      <td>0.13862</td>\n",
       "      <td>0.787870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>1.084600e-04</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.32150</td>\n",
       "      <td>-12.240</td>\n",
       "      <td>6.9704</td>\n",
       "      <td>0.70072</td>\n",
       "      <td>1.24620</td>\n",
       "      <td>0.182630</td>\n",
       "      <td>0.550100</td>\n",
       "      <td>0.16563</td>\n",
       "      <td>0.11247</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-1.406000e-05</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>-0.49587</td>\n",
       "      <td>-13.785</td>\n",
       "      <td>7.2802</td>\n",
       "      <td>-0.29896</td>\n",
       "      <td>1.82890</td>\n",
       "      <td>-0.146350</td>\n",
       "      <td>0.652270</td>\n",
       "      <td>0.30043</td>\n",
       "      <td>-0.37094</td>\n",
       "      <td>0.331040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-2.820400e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-2.01170</td>\n",
       "      <td>-14.872</td>\n",
       "      <td>7.4289</td>\n",
       "      <td>-1.19050</td>\n",
       "      <td>1.94150</td>\n",
       "      <td>-0.127370</td>\n",
       "      <td>-0.239370</td>\n",
       "      <td>0.51159</td>\n",
       "      <td>-0.57339</td>\n",
       "      <td>-0.144980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-8.058400e-05</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>-2.23190</td>\n",
       "      <td>-15.139</td>\n",
       "      <td>7.4754</td>\n",
       "      <td>-1.22780</td>\n",
       "      <td>2.07340</td>\n",
       "      <td>-0.084907</td>\n",
       "      <td>-0.187120</td>\n",
       "      <td>0.61232</td>\n",
       "      <td>-0.52491</td>\n",
       "      <td>-0.071487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-3.571900e-05</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>-1.80690</td>\n",
       "      <td>-14.663</td>\n",
       "      <td>7.6325</td>\n",
       "      <td>-1.04810</td>\n",
       "      <td>1.81870</td>\n",
       "      <td>-0.095328</td>\n",
       "      <td>-0.208120</td>\n",
       "      <td>0.52760</td>\n",
       "      <td>-0.54110</td>\n",
       "      <td>-0.187370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>1.150300e-04</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>-1.53830</td>\n",
       "      <td>-14.342</td>\n",
       "      <td>7.5361</td>\n",
       "      <td>-1.03470</td>\n",
       "      <td>1.89150</td>\n",
       "      <td>-0.138050</td>\n",
       "      <td>-0.260810</td>\n",
       "      <td>0.48861</td>\n",
       "      <td>-0.59221</td>\n",
       "      <td>-0.259780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>-4.840800e-05</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1007 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      -0.55307  -13.949  7.2276  0.81332  0.81042   0.57891  0.057156  \\\n",
       "0     -0.33518  -14.134  7.3409  0.99856  0.87461  0.806150 -0.024650   \n",
       "1     -0.53979  -14.600  7.5290  1.25120  0.65054  0.731180  0.036801   \n",
       "2     -0.96780  -15.274  7.1488  1.34540  0.83254  0.911210  0.125130   \n",
       "3      1.12280  -12.564  7.0406  0.93019  1.26220  0.117380  0.570680   \n",
       "4      1.32150  -12.240  6.9704  0.70072  1.24620  0.182630  0.550100   \n",
       "...        ...      ...     ...      ...      ...       ...       ...   \n",
       "1002  -0.49587  -13.785  7.2802 -0.29896  1.82890 -0.146350  0.652270   \n",
       "1003  -2.01170  -14.872  7.4289 -1.19050  1.94150 -0.127370 -0.239370   \n",
       "1004  -2.23190  -15.139  7.4754 -1.22780  2.07340 -0.084907 -0.187120   \n",
       "1005  -1.80690  -14.663  7.6325 -1.04810  1.81870 -0.095328 -0.208120   \n",
       "1006  -1.53830  -14.342  7.5361 -1.03470  1.89150 -0.138050 -0.260810   \n",
       "\n",
       "      0.30691  -0.28209   0.34901  ...  -1.2757e-05  -1.791e-05  0.00014175  \\\n",
       "0     0.17291  -0.33139  0.296080  ...     0.000242    0.000437    0.000095   \n",
       "1     0.25653  -0.28908  0.337880  ...     0.000097    0.000036    0.000028   \n",
       "2     0.32822  -0.29095  0.342770  ...    -0.000004   -0.000021   -0.000116   \n",
       "3     0.26390   0.13862  0.787870  ...    -0.000087   -0.000079   -0.000082   \n",
       "4     0.16563   0.11247  0.827640  ...    -0.000053    0.000042    0.000135   \n",
       "...       ...       ...       ...  ...          ...         ...         ...   \n",
       "1002  0.30043  -0.37094  0.331040  ...    -0.000291   -0.000165    0.000046   \n",
       "1003  0.51159  -0.57339 -0.144980  ...    -0.000106    0.000048    0.000157   \n",
       "1004  0.61232  -0.52491 -0.071487  ...    -0.000045   -0.000119    0.000095   \n",
       "1005  0.52760  -0.54110 -0.187370  ...     0.000158    0.000043   -0.000024   \n",
       "1006  0.48861  -0.59221 -0.259780  ...     0.000002    0.000220    0.000208   \n",
       "\n",
       "      0.00018104  2.7085e-05  1.9801e-05  1.6102e-05    1.5949e-05  \\\n",
       "0      -0.000089   -0.000200    0.000036    0.000073  2.032200e-04   \n",
       "1       0.000031    0.000075    0.000067    0.000049  5.168800e-07   \n",
       "2      -0.000025   -0.000018    0.000013   -0.000015 -2.219200e-05   \n",
       "3       0.000076    0.000107    0.000125    0.000071  1.084600e-04   \n",
       "4       0.000108    0.000032   -0.000088   -0.000079 -1.406000e-05   \n",
       "...          ...         ...         ...         ...           ...   \n",
       "1002   -0.000178    0.000093   -0.000071    0.000024 -2.820400e-06   \n",
       "1003    0.000126   -0.000015    0.000140    0.000052 -8.058400e-05   \n",
       "1004    0.000116    0.000207    0.000213    0.000094 -3.571900e-05   \n",
       "1005    0.000080   -0.000078   -0.000054   -0.000087  1.150300e-04   \n",
       "1006    0.000232   -0.000003    0.000069    0.000079 -4.840800e-05   \n",
       "\n",
       "      -1.783e-05  0  \n",
       "0       0.000179  0  \n",
       "1       0.000017  0  \n",
       "2      -0.000059  0  \n",
       "3       0.000044  0  \n",
       "4      -0.000073  0  \n",
       "...          ... ..  \n",
       "1002   -0.000012  0  \n",
       "1003   -0.000144  0  \n",
       "1004    0.000074  0  \n",
       "1005    0.000157  0  \n",
       "1006   -0.000191  0  \n",
       "\n",
       "[1007 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('C:\\\\Users\\\\singl\\\\OneDrive\\\\Documents\\\\MATLAB\\\\frontend\\\\G0PRTRAIN.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630ee8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:39].values\n",
    "\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b90d6a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The target 'y' needs to have more than 1 class. Got 1 class instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9036/319553228.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moversample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moversample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[1;34mf\"The target 'y' needs to have more than 1 class. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;34mf\"Got {np.unique(y).size} class instead\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The target 'y' needs to have more than 1 class. Got 1 class instead"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1510f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a81ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45600, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d719c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4af1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(45600,13,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b697b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e4451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4560/4560 [==============================] - 33s 7ms/step - loss: 0.1606 - accuracy: 0.7620\n",
      "Epoch 2/50\n",
      "4560/4560 [==============================] - 39s 9ms/step - loss: 0.1297 - accuracy: 0.8102\n",
      "Epoch 3/50\n",
      "4560/4560 [==============================] - 41s 9ms/step - loss: 0.1081 - accuracy: 0.8490\n",
      "Epoch 4/50\n",
      "4560/4560 [==============================] - 42s 9ms/step - loss: 0.0831 - accuracy: 0.8898\n",
      "Epoch 5/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0613 - accuracy: 0.9211\n",
      "Epoch 6/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0536 - accuracy: 0.9304\n",
      "Epoch 7/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0477 - accuracy: 0.9398\n",
      "Epoch 8/50\n",
      "4560/4560 [==============================] - 43s 9ms/step - loss: 0.0443 - accuracy: 0.9435\n",
      "Epoch 9/50\n",
      "4560/4560 [==============================] - 45s 10ms/step - loss: 0.0415 - accuracy: 0.9481\n",
      "Epoch 10/50\n",
      "4560/4560 [==============================] - 47s 10ms/step - loss: 0.0394 - accuracy: 0.9508\n",
      "Epoch 11/50\n",
      "4560/4560 [==============================] - 48s 11ms/step - loss: 0.0365 - accuracy: 0.9545\n",
      "Epoch 12/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0350 - accuracy: 0.9569\n",
      "Epoch 13/50\n",
      "4560/4560 [==============================] - 43s 9ms/step - loss: 0.0327 - accuracy: 0.9601\n",
      "Epoch 14/50\n",
      "4560/4560 [==============================] - 43s 9ms/step - loss: 0.0324 - accuracy: 0.9609\n",
      "Epoch 15/50\n",
      "4560/4560 [==============================] - 43s 9ms/step - loss: 0.0311 - accuracy: 0.9612\n",
      "Epoch 16/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0295 - accuracy: 0.9648\n",
      "Epoch 17/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0291 - accuracy: 0.9645\n",
      "Epoch 18/50\n",
      "4560/4560 [==============================] - 43s 9ms/step - loss: 0.0281 - accuracy: 0.9660\n",
      "Epoch 19/50\n",
      "4560/4560 [==============================] - 43s 9ms/step - loss: 0.0277 - accuracy: 0.9665\n",
      "Epoch 20/50\n",
      "4560/4560 [==============================] - 46s 10ms/step - loss: 0.0264 - accuracy: 0.9688\n",
      "Epoch 21/50\n",
      "4560/4560 [==============================] - 45s 10ms/step - loss: 0.0257 - accuracy: 0.9695\n",
      "Epoch 22/50\n",
      "4560/4560 [==============================] - 45s 10ms/step - loss: 0.0251 - accuracy: 0.9700\n",
      "Epoch 23/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0248 - accuracy: 0.9704\n",
      "Epoch 24/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0243 - accuracy: 0.9712\n",
      "Epoch 25/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0238 - accuracy: 0.9717\n",
      "Epoch 26/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0233 - accuracy: 0.9730\n",
      "Epoch 27/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0229 - accuracy: 0.9731\n",
      "Epoch 28/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0229 - accuracy: 0.9731\n",
      "Epoch 29/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0224 - accuracy: 0.9736\n",
      "Epoch 30/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0216 - accuracy: 0.9747\n",
      "Epoch 31/50\n",
      "4560/4560 [==============================] - 45s 10ms/step - loss: 0.0212 - accuracy: 0.9757\n",
      "Epoch 32/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0211 - accuracy: 0.9757\n",
      "Epoch 33/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0205 - accuracy: 0.9763\n",
      "Epoch 34/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0199 - accuracy: 0.9775\n",
      "Epoch 35/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0200 - accuracy: 0.9766\n",
      "Epoch 36/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0196 - accuracy: 0.9771\n",
      "Epoch 37/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0198 - accuracy: 0.9767\n",
      "Epoch 38/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0193 - accuracy: 0.9780\n",
      "Epoch 39/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0190 - accuracy: 0.9783\n",
      "Epoch 40/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0194 - accuracy: 0.9775\n",
      "Epoch 41/50\n",
      "4560/4560 [==============================] - 43s 10ms/step - loss: 0.0182 - accuracy: 0.9797\n",
      "Epoch 42/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0183 - accuracy: 0.9793\n",
      "Epoch 43/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0185 - accuracy: 0.9789\n",
      "Epoch 44/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0177 - accuracy: 0.9798\n",
      "Epoch 45/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0175 - accuracy: 0.9801\n",
      "Epoch 46/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0173 - accuracy: 0.9801\n",
      "Epoch 47/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0168 - accuracy: 0.9811\n",
      "Epoch 48/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0168 - accuracy: 0.9808\n",
      "Epoch 49/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0172 - accuracy: 0.9809\n",
      "Epoch 50/50\n",
      "4560/4560 [==============================] - 44s 10ms/step - loss: 0.0167 - accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d69b824dc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (13, 1)))\n",
    "\n",
    "#regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (67, 1)))\n",
    "\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "#regressor.fit(X_train, y_train, validation_data= (X_test,y_test),epochs = 50, batch_size = 32)\n",
    "regressor.fit(X, y, epochs = 50, batch_size = 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed38afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtf=pd.read_csv('GFdfeval.csv')\n",
    "dtf\n",
    "Xt = dtf.iloc[:, 0:13].values\n",
    "\n",
    "yt = dtf.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45e8d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Xt = np.reshape(Xt, (Xt.shape[0], Xt.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ea380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi Chakravarty\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "prediction1=regressor.predict_classes(Xt)\n",
    "#prediction1=regressor.predict((Xt) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39c92df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.5817\n",
      "accuracy: 58.17%\n"
     ]
    }
   ],
   "source": [
    "score=regressor.evaluate(Xt, yt)\n",
    "print(\"%s: %.2f%%\" % (regressor.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1888512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[  29   45]\n",
      " [1079 1534]]\n",
      "Accuracy: 0.581690\n",
      "Precision: 0.971501\n",
      "Recall: 0.587065\n",
      "F1 score: 0.731870\n",
      "1\n",
      "EER =  0.4129353233830846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "results=confusion_matrix(yt, prediction1)\n",
    "print(\"confusion matrix\", results)\n",
    "\n",
    "accuracy = accuracy_score(yt, prediction1)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(yt, prediction1)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(yt, prediction1)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(yt, prediction1)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(yt,prediction1, pos_label=1)\n",
    "fnr = 1 - tpr\n",
    "eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "EER = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "print(eer_threshold)\n",
    "print(\"EER = \", EER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(yt,prediction1, pos_label=1)\n",
    "fnr = 1 - tpr\n",
    "eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "EER = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "print(eer_threshold)\n",
    "print(\"EER = \", EER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3948af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
